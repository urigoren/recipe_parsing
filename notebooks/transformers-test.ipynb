{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "yellow-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: module: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `module'\n",
      "/bin/bash: switchml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `switchml'\n",
      "Requirement already satisfied: transformers in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (4.4.2)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.8.0+cu111)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.5.0)\n",
      "Requirement already satisfied: rouge_score in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: jiwer in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.8.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (4.49.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.0.7)\n",
      "Requirement already satisfied: dill in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.19.2)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.2.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: python-Levenshtein in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from jiwer) (0.12.2)\n",
      "Requirement already satisfied: absl-py in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from rouge_score) (3.5)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from nltk->rouge_score) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from python-Levenshtein->jiwer) (49.6.0.post20210108)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers torch datasets rouge_score jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "reliable-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric, list_datasets, list_metrics\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "compressed-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints\n",
    "# # forward\n",
    "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "# outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
    "# # training\n",
    "# outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n",
    "# loss, logits = outputs.loss, outputs.logits\n",
    "# # save and load from pretrained\n",
    "# model.save_pretrained(\"bert2bert\")\n",
    "# model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n",
    "# # generation\n",
    "# generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "departmental-surge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-711f9814101edcda\n",
      "Reusing dataset csv (/home/ubuntu/.cache/huggingface/datasets/csv/default-711f9814101edcda/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-711f9814101edcda\n",
      "Reusing dataset csv (/home/ubuntu/.cache/huggingface/datasets/csv/default-711f9814101edcda/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_seq', 'output_seq'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_seq', 'output_seq'],\n",
      "    num_rows: 451\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'seq2seq_4335716.csv'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "val_data = load_dataset('csv', data_files=csv_file, split='train[90%:]')\n",
    "train_data = load_dataset('csv', data_files=csv_file, split='train[:90%]')\n",
    "print(val_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "european-mechanics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STOP_USING TSPATULA STOVE_MED CHEF_CHECK LBOIL STOVE_MED',\n",
       " 'USE TSPATULA STOVE_MED PUT IN2e0UIJI STOVE_MED',\n",
       " 'MOVE_CONTENTS STOVE_MED STOVE_MEDLOW USE TPAN STOVE_MEDLOW STOP_USING TPAN STOVE_MED STOP_USING TSPATULA STOVE_MED CHEF_CHECK LTIME STOVE_MEDLOW',\n",
       " 'USE TSPATULA STOVE_MEDLOW PUT IzrIHcuDJ STOVE_MEDLOW PUT IrXTUISnn STOVE_MEDLOW',\n",
       " 'MOVE_CONTENTS STOVE_MED SERVE MOVE_CONTENTS STOVE_MEDLOW SERVE STOP_USING TPAN STOVE_MEDLOW STOP_USING TSPATULA STOVE_MEDLOW PUT Is_lS1dmt SERVE',\n",
       " 'USE TBOWL COUNTER1 PUT IzrIHcuDJ COUNTER1 PUT IzPYYTv5e COUNTER1 PUT IdCkp3LYx COUNTER1 PUT Is_lS1dmt COUNTER1',\n",
       " 'MOVE_CONTENTS COUNTER1 OVEN_MED USE TBAKE_DISH OVEN_MED STOP_USING TBOWL COUNTER1 CHEF_CHECK LTIME OVEN_MED',\n",
       " 'MOVE_CONTENTS OVEN_MED COUNTER1 USE TPAN COUNTER1 STOP_USING TBAKE_DISH OVEN_MED PUT IoskwJgPz COUNTER1 PUT IlGA3C7DK COUNTER1 PUT IraFb0aXd COUNTER1 PUT I6WhEfvid COUNTER1',\n",
       " 'MOVE_CONTENTS COUNTER1 STOVE_MED MOVE_CONTENTS OVEN_MED STOVE_MED USE TPAN STOVE_MED STOP_USING TPAN COUNTER1 CHEF_CHECK LMELT STOVE_MED',\n",
       " 'MOVE_CONTENTS COUNTER1 OVEN_HI MOVE_CONTENTS STOVE_MED OVEN_HI MOVE_CONTENTS OVEN_MED OVEN_HI USE TPAN OVEN_HI STOP_USING TPAN STOVE_MED CHEF_CHECK LTIME OVEN_HI',\n",
       " 'MOVE_CONTENTS COUNTER1 SERVE MOVE_CONTENTS OVEN_HI SERVE MOVE_CONTENTS STOVE_MED SERVE MOVE_CONTENTS OVEN_MED SERVE USE TPAN SERVE STOP_USING TPAN OVEN_HI',\n",
       " 'USE TSKILLET STOVE_LOW USE TSPATULA STOVE_LOW PUT IzPYYTv5e STOVE_LOW PUT Iz0wiMjVJ STOVE_LOW PUT I10_d4pRP STOVE_LOW CHEF_CHECK LBROWN STOVE_LOW',\n",
       " 'MOVE_CONTENTS STOVE_LOW COUNTER1 USE TPLATE COUNTER1 STOP_USING TSPATULA STOVE_LOW STOP_USING TSKILLET STOVE_LOW PUT IlGA3C7DK COUNTER1 PUT IzrIHcuDJ COUNTER1 PUT I5IP2D1SP COUNTER1 PUT IBhyfTisH COUNTER1',\n",
       " 'MOVE_CONTENTS COUNTER1 STOVE_MEDLOW MOVE_CONTENTS STOVE_LOW STOVE_MEDLOW USE TSKILLET STOVE_MEDLOW STOP_USING TPLATE COUNTER1 CHEF_CHECK LBOIL STOVE_MEDLOW CHEF_CHECK LTIME STOVE_MEDLOW',\n",
       " 'MOVE_CONTENTS COUNTER1 SERVE MOVE_CONTENTS STOVE_LOW SERVE MOVE_CONTENTS STOVE_MEDLOW SERVE USE TBOWL SERVE STOP_USING TSKILLET STOVE_MEDLOW',\n",
       " 'PUT I23cXSqNN COUNTER1 PUT Is_lS1dmt COUNTER1 PUT Iz0wiMjVJ COUNTER1',\n",
       " 'USE TBOWL COUNTER1',\n",
       " 'PUT ITgJlN6fT COUNTER1 PUT Ine5Fok7_ COUNTER1 CHEF_CHECK LTIME COUNTER1',\n",
       " 'USE TPAN COUNTER1 STOP_USING TBOWL COUNTER1 CHEF_CHECK LSIDES COUNTER1',\n",
       " 'CHEF_CHECK LTEXTURE COUNTER1',\n",
       " 'MOVE_CONTENTS COUNTER1 SERVE USE TPLATE SERVE STOP_USING TPAN COUNTER1',\n",
       " 'USE TPAN STOVE_HI PUT IlGA3C7DK STOVE_HI CHEF_CHECK LBOIL STOVE_HI',\n",
       " 'MOVE_CONTENTS STOVE_HI STOVE_MED USE TPAN STOVE_MED STOP_USING TPAN STOVE_HI PUT Iz0wiMjVJ STOVE_MED',\n",
       " 'CHEF_CHECK LTEXTURE STOVE_MED CHEF_CHECK LINSIDE STOVE_MED',\n",
       " 'MOVE_CONTENTS STOVE_MED STOVE_LOW MOVE_CONTENTS STOVE_HI STOVE_LOW USE TPAN STOVE_LOW STOP_USING TPAN STOVE_MED',\n",
       " 'PUT IzPYYTv5e STOVE_LOW PUT IRghrxgvV STOVE_LOW PUT IraFb0aXd STOVE_LOW PUT IBhyfTisH STOVE_LOW PUT IQxauuYZx STOVE_LOW',\n",
       " 'CHEF_CHECK LTEXTURE STOVE_LOW CHEF_CHECK LTIME STOVE_LOW',\n",
       " 'USE TSKILLET STOVE_MEDLOW PUT I10_d4pRP STOVE_MEDLOW',\n",
       " 'PUT I1IusNkrV STOVE_MEDLOW PUT Iz0wiMjVJ STOVE_MEDLOW CHEF_CHECK LSIDES STOVE_MEDLOW',\n",
       " 'PUT IraFb0aXd STOVE_MEDLOW PUT IfztrkJfj STOVE_MEDLOW PUT Is_lS1dmt STOVE_MEDLOW PUT IzrIHcuDJ STOVE_MEDLOW PUT IzDRSSnRu STOVE_MEDLOW PUT I6WhEfvid STOVE_MEDLOW PUT IaeZBN7Aq STOVE_MEDLOW CHEF_CHECK LTIME STOVE_MEDLOW',\n",
       " 'MOVE_CONTENTS STOVE_MEDLOW SERVE USE TSKILLET SERVE STOP_USING TSKILLET STOVE_MEDLOW',\n",
       " 'USE TSKILLET STOVE_MED PUT IJAkkJI6_ STOVE_MED',\n",
       " 'PUT IpGQkiesK STOVE_MED PUT IT_JYeEg2 STOVE_MED PUT Iz0wiMjVJ STOVE_MED',\n",
       " 'USE TSPATULA STOVE_MED CHEF_CHECK LTEXTURE STOVE_MED',\n",
       " 'PUT IrXTUISnn STOVE_MED PUT Is_lS1dmt STOVE_MED PUT I_qDx9v7e STOVE_MED PUT IxUjqmQ6R STOVE_MED CHEF_CHECK LTEXTURE STOVE_MED',\n",
       " 'MOVE_CONTENTS STOVE_MED STOVE_LOW USE TSKILLET STOVE_LOW STOP_USING TSKILLET STOVE_MED STOP_USING TSPATULA STOVE_MED',\n",
       " 'CHEF_CHECK LTIME STOVE_LOW CHEF_CHECK LSOLID STOVE_LOW',\n",
       " 'USE TGRATER COUNTER1 PUT IjuICeYOR COUNTER1',\n",
       " 'USE TGRATER COUNTER2 PUT IT_JYeEg2 COUNTER2',\n",
       " 'USE TSKILLET STOVE_MEDHI PUT Ixa8FQ0B6 STOVE_MEDHI',\n",
       " 'REMOVE IjuICeYOR COUNTER1 REMOVE IT_JYeEg2 COUNTER2 REMOVE Ixa8FQ0B6 STOVE_MEDHI STOP_USING TGRATER COUNTER2 STOP_USING TSKILLET STOVE_MEDHI PUT IT_JYeEg2 COUNTER1',\n",
       " 'REMOVE IT_JYeEg2 COUNTER1 USE TSKILLET STOVE_MEDHI STOP_USING TGRATER COUNTER1 PUT Ixa8FQ0B6 STOVE_MEDHI PUT IjuICeYOR STOVE_MEDHI CHEF_CHECK LTIME STOVE_MEDHI',\n",
       " 'REMOVE Ixa8FQ0B6 STOVE_MEDHI REMOVE IjuICeYOR STOVE_MEDHI USE TGRATER COUNTER1 STOP_USING TSKILLET STOVE_MEDHI PUT IT_JYeEg2 COUNTER1',\n",
       " 'USE TPAN STOVE_MED USE TCOOK_SPRAY STOVE_MED PUT Iz0wiMjVJ STOVE_MED',\n",
       " 'PUT I1IusNkrV STOVE_MED PUT IzPYYTv5e STOVE_MED PUT IAF0F3ilI STOVE_MED CHEF_CHECK LTIME STOVE_MED',\n",
       " 'PUT IsxiK2rPw STOVE_MED CHEF_CHECK LTIME STOVE_MED',\n",
       " 'USE TFOIL STOVE_MED PUT IzrIHcuDJ STOVE_MED PUT Is_lS1dmt STOVE_MED CHEF_CHECK LTIME STOVE_MED',\n",
       " 'PUT IN2e0UIJI STOVE_MED CHEF_CHECK LTIME STOVE_MED',\n",
       " 'PUT I_qDx9v7e STOVE_MED CHEF_CHECK LTIME STOVE_MED',\n",
       " 'MOVE_CONTENTS STOVE_MED SERVE USE TCOOK_SPRAY SERVE USE TPAN SERVE USE TFOIL SERVE STOP_USING TCOOK_SPRAY STOVE_MED STOP_USING TPAN STOVE_MED STOP_USING TFOIL STOVE_MED']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['output_seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "processed-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # 4 but change to 16 for full training\n",
    "encoder_max_length = 128\n",
    "decoder_max_length = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    tok_params = {\n",
    "        'padding': 'max_length',\n",
    "        'truncation': True,\n",
    "        'max_length': encoder_max_length,\n",
    "    }\n",
    "    inputs = tokenizer(batch['input_seq'], **tok_params)\n",
    "    outputs = tokenizer(batch['output_seq'], **tok_params)\n",
    "\n",
    "    batch['input_ids'] = inputs.input_ids\n",
    "    batch['attention_mask'] = inputs.attention_mask\n",
    "    batch['decoder_input_ids'] = outputs.input_ids\n",
    "    batch['decoder_attention_mask'] = outputs.attention_mask\n",
    "    batch['labels'] = outputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch['labels'] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch['labels']\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-investigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab30783579f04be09c6ed4483f985ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/csv/default-711f9814101edcda/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-9fd16f8276b06b93.arrow\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/2500 : < :, Epoch 0.03/87]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only use 32 training examples for notebook - COMMENT LINE FOR FULL TRAINING\n",
    "# train_data = train_data.select(range(32))\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "#     remove_columns=['name', 'note'],\n",
    ")\n",
    "train_data.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",
    ")\n",
    "\n",
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "# val_data = val_data.select(range(16))\n",
    "\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "#     remove_columns=['name', 'note'],\n",
    ")\n",
    "val_data.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",
    ")\n",
    "\n",
    "ed_model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')\n",
    "\n",
    "# set special tokens\n",
    "ed_model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "ed_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "ed_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "ed_model.config.vocab_size = ed_model.config.decoder.vocab_size\n",
    "ed_model.config.max_length = 142\n",
    "ed_model.config.min_length = 56\n",
    "ed_model.config.no_repeat_ngram_size = 3\n",
    "ed_model.config.early_stopping = True\n",
    "ed_model.config.length_penalty = 2.0\n",
    "ed_model.config.num_beams = 4\n",
    "\n",
    "\n",
    "# load wer for validation\n",
    "wer = load_metric('wer')\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer_output = wer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\n",
    "        'wer': round(wer_output, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./',\n",
    "    evaluation_strategy='steps',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=500,  # 2 or set to 1000 for full training\n",
    "    save_steps=500,  # 16 or set to 500 for full training\n",
    "    eval_steps=500,  # 4 or set to 8000 for full training\n",
    "    warmup_steps=500,  # 1 or set to 2000 for full training\n",
    "    max_steps=2500,  # 16 or comment for full training\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=ed_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-defense",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_model.save_pretrained(\"recipe_bert_seq2seq\")\n",
    "!aws s3 cp --recursive recipe_bert_seq2seq s3://uatt-data/uri/\n",
    "!rm -rf recipe_bert_seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-robin",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"Cook chicken until golden brown .\"\n",
    "input_ids = torch.tensor(tokenizer.encode(input_text, add_special_tokens=True)).unsqueeze(0).to(device)  # Batch size 1\n",
    "generated = ed_model.generate(input_ids, decoder_start_token_id=ed_model.config.decoder.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = generated.to('cpu').numpy().reshape(-1)\n",
    "tokenizer.convert_ids_to_tokens(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp transformers-test.ipynb s3://uatt-notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-trading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
